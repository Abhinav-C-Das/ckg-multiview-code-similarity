Good. This is the **right moment to freeze the system** and write the README like a serious research artifact.

Below is a **clean, authoritative README.md** you can drop into the repo **as-is**.
It documents **what exists, why it exists, and how to run everything** without tribal knowledge.

---

# ğŸ“˜ README â€” Code Similarity Pipeline (Baseline + WL)

## 1. Project Overview

This project implements a **research-grade code similarity pipeline** for comparing **student solutions against reference solutions**, using:

* **Baseline numeric features** (structural, semantic, behavioral)
* **AST-based Weisfeilerâ€“Lehman (WL) kernel**
* Clean **CPG reuse** via Joern
* **Separated pipelines** for feature extraction, similarity computation, and evaluation
* **Top-k retrieval evaluation** for diagnostic analysis

The design intentionally keeps WL **weak and syntactic**, to motivate stronger semantic representations (e.g., CES).

---

## 2. Directory Structure (Final)

```
ckg_f2/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ p1/
â”‚       â”œâ”€â”€ ref/
â”‚       â”‚   â”œâ”€â”€ ref1.c
â”‚       â”‚   â”œâ”€â”€ ref2.c
â”‚       â”‚   â””â”€â”€ ref3.c
â”‚       â””â”€â”€ s/
â”‚           â”œâ”€â”€ s1.c
â”‚           â”œâ”€â”€ s2.c
â”‚           â”œâ”€â”€ s3.c
â”‚           â””â”€â”€ s_semantic.c
â”‚
â”œâ”€â”€ outputs/                         # Joern + feature outputs (CPG reuse)
â”‚   â””â”€â”€ p1/ref/ref1/
â”‚       â”œâ”€â”€ canonical.json
â”‚       â”œâ”€â”€ structural.json
â”‚       â”œâ”€â”€ semantic.json
â”‚       â”œâ”€â”€ behavioral.json
â”‚       â””â”€â”€ combined_features.json
â”‚
â”œâ”€â”€ vectors/
â”‚   â”œâ”€â”€ baseline/
â”‚   â”‚   â”œâ”€â”€ p1_ref_ref1.vec
â”‚   â”‚   â”œâ”€â”€ p1_ref_ref1.norm.vec
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ wl/
â”‚       â”œâ”€â”€ wl_vocab.json
â”‚       â”œâ”€â”€ p1_ref_ref1.vec
â”‚       â”œâ”€â”€ p1_s_s1.vec
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ cpg/scripts/
â”‚   â”œâ”€â”€ preprocess/canonicalize.sc
â”‚   â”œâ”€â”€ structural/basic_structural.sc
â”‚   â”œâ”€â”€ semantic/basic_semantic.sc
â”‚   â”œâ”€â”€ behavioral/basic_behavioral.sc
â”‚   â””â”€â”€ wl/wl_ast.sc
â”‚
â”œâ”€â”€ similarity/
â”‚   â”œâ”€â”€ aggregate_baseline.py
â”‚   â”œâ”€â”€ vectorize_features.py
â”‚   â”œâ”€â”€ normalize.py
â”‚   â”œâ”€â”€ build_wl_vocab.py
â”‚   â”œâ”€â”€ vectorize_wl.py
â”‚   â”œâ”€â”€ normalize_wl.py
â”‚   â”œâ”€â”€ wl_similarity.py
â”‚   â””â”€â”€ similarity.py
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ run_baseline_similarity_matrix.py
â”‚   â”œâ”€â”€ run_wl_similarity_matrix.py
â”‚   â”œâ”€â”€ aggregate_baseline_wl.py
â”‚   â”œâ”€â”€ run_topk_retrieval_evaluation.py
â”‚   â”œâ”€â”€ ground_truth.json
â”‚   â”œâ”€â”€ similarity_matrix.json
â”‚   â”œâ”€â”€ wl_similarity_matrix.json
â”‚   â””â”€â”€ combined_similarity.json
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_joern.sh
â”‚   â”œâ”€â”€ run_baseline_pipeline.sh
â”‚   â””â”€â”€ pipeline/
â”‚       â””â”€â”€ run_wl_pipeline.sh
â”‚
â”œâ”€â”€ run_full_pipeline.sh
â””â”€â”€ README.md
```

---

## 3. Core Concepts

### Roles

* **ref/** â†’ reference (correct) solutions
* **s/** â†’ student submissions

### Problems

Each subdirectory under `data/` is treated as an **independent problem** (e.g., `p1`, `p2`, â€¦).

### CPG Reuse

* Each programâ€™s CPG is built **once**
* Stored implicitly via Joern execution
* Reused automatically on subsequent runs

---

## 4. Script Responsibilities

### 4.1 Joern / Feature Extraction

#### `experiments/run_joern.sh`

* Builds CPGs using Joern
* Extracts:

  * structural features
  * semantic features (local defâ€“use)
  * behavioral features
* Writes outputs to `outputs/<problem>/<role>/<program>/`
* **Reuses existing CPGs automatically**

Run directly if needed:

```bash
./experiments/run_joern.sh
```

---

### 4.2 Baseline Pipeline

#### `experiments/run_baseline_pipeline.sh`

Per program:

1. Aggregates numeric features (`aggregate_baseline.py`)
2. Vectorizes (`vectorize_features.py`)
3. L2-normalizes (`normalize.py`)
4. Computes baseline similarity matrix

Produces:

```
evaluation/similarity_matrix.json
```

---

#### `evaluation/run_baseline_similarity_matrix.py`

* Computes pairwise similarity between:

  * normalized student vectors
  * normalized reference vectors
* Uses `similarity/similarity.py`
* Output: baseline similarity matrix

---

### 4.3 WL Pipeline

#### `experiments/pipeline/run_wl_pipeline.sh`

Runs the full WL pipeline:

1. WL AST extraction (`wl_ast.sc`)
2. WL vocabulary building
3. WL vectorization
4. WL similarity matrix computation

Produces:

```
evaluation/wl_similarity_matrix.json
```

WL uses:

* L2 normalization (`normalize_wl.py`)
* Cosine similarity (`wl_similarity.py`)
* **AST only (intentionally weak baseline)**

---

### 4.4 Aggregation

#### `evaluation/aggregate_baseline_wl.py`

* Aligns baseline and WL similarities
* Produces a joint view per `(student, reference)`

Output:

```
evaluation/combined_similarity.json
```

Example entry:

```json
{
  "baseline": 0.81,
  "wl": 0.99
}
```

No ranking or evaluation is done here.

---

## 5. Evaluation

### 5.1 Top-k Retrieval Evaluation

#### `evaluation/run_topk_retrieval_evaluation.py`

* Computes Top-k accuracy (k = 1, 3, 5)
* Uses:

  * similarity matrix (WL or baseline)
  * `evaluation/ground_truth.json`
* Normalizes identifiers internally (safe and robust)

Run:

```bash
python3 evaluation/run_topk_retrieval_evaluation.py
```

Expected WL behavior:

* Top-1 â‰ˆ 0
* Top-k may be 0 or >0 depending on ground truth
* This is **diagnostic**, not a failure

---

## 6. Full Pipeline (End-to-End)

### `run_full_pipeline.sh`

Runs everything in correct order:

1. Joern feature extraction (CPG reuse)
2. Baseline pipeline + similarity
3. WL pipeline + similarity
4. Aggregation of baseline + WL

Run:

```bash
./run_full_pipeline.sh
```

After completion, you should have:

```
evaluation/
â”œâ”€â”€ similarity_matrix.json
â”œâ”€â”€ wl_similarity_matrix.json
â””â”€â”€ combined_similarity.json
```

---

## 7. Clean Reset (Recommended)

To remove all generated artifacts:

```bash
rm -rf outputs vectors
rm -f evaluation/*similarity*.json
rm -f evaluation/combined_similarity.json
```

Then rerun:

```bash
./run_full_pipeline.sh
```

---

## 8. Design Philosophy (Important)

* **WL is intentionally weak**

  * AST-only
  * No CFG / PDG / canonicalization
* Poor Top-1 accuracy is **expected and meaningful**
* Evaluation is **diagnostic**, not competitive
* Stronger methods (e.g., CES) should clearly outperform both baselines

---

## 9. Extending the System

To add a new method (e.g., CES):

1. Create a pipeline script analogous to `run_wl_pipeline.sh`
2. Produce `evaluation/ces_similarity_matrix.json`
3. Reuse:

   * `aggregate_baseline_wl.py` (extend if needed)
   * `run_topk_retrieval_evaluation.py`

No refactor required.

---

## 10. Final Notes

* Pipelines are **frozen**
* Do not tune WL
* Do not change ground truth
* Focus next on:

  * Top-k plots
  * Comparison tables
  * Experimental section writing

---

If you want, next we can:

* generate plots
* design result tables
* draft the experimental section text

Just say **â€œmove to resultsâ€**.
